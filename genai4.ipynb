{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a33cd9",
   "metadata": {
    "height": 949
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n",
      "==================================================\n",
      "assistant: The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results on these extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "==================================================\n",
      "assistant: Here are summaries of Self-RAG and LongLoRA:\n",
      "\n",
      "1. Self-RAG is a framework that enhances the quality and factuality of large language models by incorporating retrieval and self-reflection mechanisms. It enables language models to generate text informed by retrieved passages when necessary and evaluate the output using special tokens known as reflection tokens. This approach ensures that the generated content is fully supported by the retrieved passages, leading to improved generation quality without compromising creativity and versatility.\n",
      "\n",
      "2. LongLoRA is an efficient method for extending the context length of Large Language Models (LLMs) with limited computational cost. It combines shifted sparse attention (S2-Attn) with LoRA to enable fine-tuning to longer context lengths while reducing the need for extensive computational resources. Despite aiming to save trainable parameters and memory costs compared to full fine-tuning, LongLoRA may exhibit a noticeable performance gap when adapting LLMs from short to long context lengths, especially as the target context length increases. The method introduces S2-Attn to approximate standard self-attention patterns during training, making it easy to implement and retaining the original attention architecture during inference. Additionally, LongLoRA bridges the gap between LoRA and full fine-tuning through trainable normalization and embedding, allowing for the efficient extension of LLMs to larger context lengths.\n",
      "==================================================\n",
      "assistant: The evaluation dataset used in MetaGPT includes HumanEval, MBPP, and SoftwareDev. \n",
      "\n",
      "On the other hand, the evaluation dataset used in SWE-Bench includes diverse instruction-following input-output pairs sampled from Open-Instruct processed data, as well as knowledge-intensive datasets such as PopQA, PubHealth, and ASQA. In total, 150k instruction-output pairs are utilized for training in SWE-Bench.\n"
     ]
    }
   ],
   "source": [
    "from helper import get_openai_api_key\n",
    "OPENAI_API_KEY = get_openai_api_key()\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n",
    "\n",
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n",
    "\n",
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n",
    "\n",
    "# âœ… Set verbose=False to suppress function logs\n",
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools,\n",
    "    llm=llm,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "agent = AgentRunner(agent_worker)\n",
    "\n",
    "# === QUERY 1 ===\n",
    "response1 = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")\n",
    "print(\"=\" * 50)\n",
    "print(response1)\n",
    "\n",
    "# === QUERY 2 ===\n",
    "response2 = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(\"=\" * 50)\n",
    "print(response2)\n",
    "\n",
    "# === QUERY 3 ===\n",
    "response3 = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(\"=\" * 50)\n",
    "print(response3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e2a189",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
